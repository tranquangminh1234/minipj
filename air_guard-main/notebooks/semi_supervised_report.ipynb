{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c25e4b",
   "metadata": {},
   "source": [
    "# Semi-supervised vs Supervised — Summary Report \n",
    "\n",
    "Notebook này đọc các file metrics/predictions đã sinh ra và tạo báo cáo so sánh:\n",
    "1) So sánh Accuracy & F1-macro giữa Baseline / Self-training / Co-training.\n",
    "2) Biểu đồ động lực (pseudo-labels theo vòng lặp).\n",
    "3) Cảnh báo AQI theo trạm (đếm số lần alert) + ví dụ time series cho 1 trạm.\n",
    "\n",
    "Gợi ý mini project: chạy nhiều cấu hình (LABEL_MISSING_FRACTION, TAU) -> lưu metrics theo tên khác nhau, rồi đưa list paths vào PARAMETERS để notebook tự tổng hợp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3ed7d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "BASELINE_METRICS_PATH = \"data/processed/metrics.json\"\n",
    "\n",
    "# Semi-supervised artefacts produced by this repo\n",
    "SELF_METRICS_PATHS = [\"data/processed/metrics_self_training.json\"]\n",
    "CO_METRICS_PATHS = [\"data/processed/metrics_co_training.json\"]\n",
    "\n",
    "# Optional: station-level storytelling (alerts/predictions)\n",
    "BASELINE_PRED_PATH = \"data/processed/predictions_sample.csv\"\n",
    "SELF_ALERTS_PATH = \"data/processed/alerts_self_training_sample.csv\"\n",
    "CO_ALERTS_PATH = \"data/processed/alerts_co_training_sample.csv\"\n",
    "\n",
    "# Pick a station to visualize (None -> auto choose top-alert station)\n",
    "STATION_TO_PLOT = None\n",
    "\n",
    "# For timeline plots (avoid rendering too many points)\n",
    "MAX_ROWS_PLOT = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# -------- paths (robust for papermill + interactive) --------\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "\n",
    "def _resolve(path_str: str) -> Path:\n",
    "    return (PROJECT_ROOT / path_str).resolve()\n",
    "\n",
    "def load_json(path_str: str) -> Optional[Dict[str, Any]]:\n",
    "    p = _resolve(path_str)\n",
    "    if not p.exists():\n",
    "        print(\"Missing:\", p)\n",
    "        return None\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_csv(path_str: str) -> Optional[pd.DataFrame]:\n",
    "    p = _resolve(path_str)\n",
    "    if not p.exists():\n",
    "        print(\"Missing:\", p)\n",
    "        return None\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "# -------- metrics normalization --------\n",
    "def normalize_metrics(obj: Optional[Dict[str, Any]], method: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Return a flat dict for summary table.\n",
    "    - baseline (supervised) often stores metrics as a flat dict.\n",
    "    - semi-supervised stores {\"test_metrics\": {...}, \"history\": [...], \"config\": {...}}.\n",
    "    \"\"\"\n",
    "    if obj is None:\n",
    "        return None\n",
    "\n",
    "    row: Dict[str, Any] = {\"method\": method}\n",
    "\n",
    "    if \"test_metrics\" in obj and isinstance(obj[\"test_metrics\"], dict):\n",
    "        row.update(obj[\"test_metrics\"])\n",
    "        # bring tau/cutoff if present\n",
    "        cfg = obj.get(\"ct_cfg\") or obj.get(\"st_cfg\") or obj.get(\"config\") or {}\n",
    "        if isinstance(cfg, dict):\n",
    "            if \"tau\" in cfg: row[\"tau\"] = cfg[\"tau\"]\n",
    "            if \"cutoff\" in cfg: row[\"cutoff\"] = cfg[\"cutoff\"]\n",
    "        if \"cutoff\" in obj: row[\"cutoff\"] = obj[\"cutoff\"]\n",
    "        if \"tau\" in obj: row[\"tau\"] = obj[\"tau\"]\n",
    "    else:\n",
    "        # baseline flat metrics\n",
    "        if isinstance(obj, dict):\n",
    "            row.update(obj)\n",
    "\n",
    "    return row\n",
    "\n",
    "# -------- load artefacts --------\n",
    "baseline_obj = load_json(BASELINE_METRICS_PATH)\n",
    "self_objs = [load_json(p) for p in SELF_METRICS_PATHS]\n",
    "co_objs = [load_json(p) for p in CO_METRICS_PATHS]\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "b = normalize_metrics(baseline_obj, \"baseline_supervised\")\n",
    "if b is not None:\n",
    "    rows.append(b)\n",
    "\n",
    "for obj in self_objs:\n",
    "    r = normalize_metrics(obj, \"self_training\")\n",
    "    if r is not None:\n",
    "        rows.append(r)\n",
    "\n",
    "for obj in co_objs:\n",
    "    r = normalize_metrics(obj, \"co_training\")\n",
    "    if r is not None:\n",
    "        rows.append(r)\n",
    "\n",
    "if not rows:\n",
    "    raise FileNotFoundError(\n",
    "        \"No metrics files found. Expected at least one of: \"\n",
    "        f\"{BASELINE_METRICS_PATH}, {SELF_METRICS_PATHS}, {CO_METRICS_PATHS}\"\n",
    "    )\n",
    "\n",
    "dfm = pd.DataFrame(rows)\n",
    "\n",
    "# Prefer these columns if present\n",
    "preferred = [\"method\", \"tau\", \"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\", \"n_train\", \"n_test\", \"cutoff\"]\n",
    "cols = [c for c in preferred if c in dfm.columns] + [c for c in dfm.columns if c not in preferred]\n",
    "dfm = dfm[cols]\n",
    "\n",
    "# sort for a clean story\n",
    "method_order = {\"baseline_supervised\": 0, \"self_training\": 1, \"co_training\": 2}\n",
    "dfm[\"_mord\"] = dfm[\"method\"].map(method_order).fillna(99)\n",
    "if \"tau\" in dfm.columns:\n",
    "    dfm = dfm.sort_values([\"_mord\", \"tau\"], na_position=\"last\")\n",
    "else:\n",
    "    dfm = dfm.sort_values([\"_mord\"])\n",
    "dfm = dfm.drop(columns=[\"_mord\"])\n",
    "\n",
    "display(dfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217317ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Accuracy & F1-macro (if available)\n",
    "plot_df = dfm.copy()\n",
    "\n",
    "have_acc = \"accuracy\" in plot_df.columns\n",
    "have_f1 = \"f1_macro\" in plot_df.columns\n",
    "\n",
    "if not have_acc and not have_f1:\n",
    "    print(\"No 'accuracy' or 'f1_macro' columns found in metrics files.\")\n",
    "else:\n",
    "    plot_df[\"label\"] = plot_df[\"method\"].astype(str)\n",
    "    if \"tau\" in plot_df.columns:\n",
    "        plot_df[\"label\"] = plot_df[\"label\"] + plot_df[\"tau\"].apply(lambda x: \"\" if pd.isna(x) else f\" (tau={x})\")\n",
    "\n",
    "    x = np.arange(len(plot_df))\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    width = 0.4\n",
    "    if have_acc and have_f1:\n",
    "        ax.bar(x - width/2, plot_df[\"accuracy\"].values, width=width, label=\"Accuracy\")\n",
    "        ax.bar(x + width/2, plot_df[\"f1_macro\"].values, width=width, label=\"F1-macro\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "    elif have_acc:\n",
    "        ax.bar(x, plot_df[\"accuracy\"].values, width=0.6, label=\"Accuracy\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "    else:\n",
    "        ax.bar(x, plot_df[\"f1_macro\"].values, width=0.6, label=\"F1-macro\")\n",
    "        ax.set_ylim(0, 1.0)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(plot_df[\"label\"].tolist(), rotation=25, ha=\"right\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"AQI classification: supervised vs semi-supervised\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storytelling: pseudo-label dynamics (if history exists)\n",
    "\n",
    "def load_history(obj: Optional[Dict[str, Any]]) -> Optional[pd.DataFrame]:\n",
    "    if obj is None:\n",
    "        return None\n",
    "    hist = obj.get(\"history\")\n",
    "    if isinstance(hist, list) and len(hist) > 0:\n",
    "        return pd.DataFrame(hist)\n",
    "    return None\n",
    "\n",
    "h_self = load_history(self_objs[0]) if len(self_objs) else None\n",
    "h_co = load_history(co_objs[0]) if len(co_objs) else None\n",
    "\n",
    "def plot_dynamics(h: pd.DataFrame, title: str):\n",
    "    if h is None or h.empty:\n",
    "        return\n",
    "    if \"iter\" not in h.columns:\n",
    "        # try fallback\n",
    "        if \"iteration\" in h.columns:\n",
    "            h = h.rename(columns={\"iteration\": \"iter\"})\n",
    "        else:\n",
    "            print(f\"History for {title} has no 'iter' column.\")\n",
    "            return\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    if \"val_f1_macro\" in h.columns:\n",
    "        ax1.plot(h[\"iter\"], h[\"val_f1_macro\"], marker=\"o\")\n",
    "        ax1.set_ylabel(\"Val F1-macro\")\n",
    "    elif \"val_accuracy\" in h.columns:\n",
    "        ax1.plot(h[\"iter\"], h[\"val_accuracy\"], marker=\"o\")\n",
    "        ax1.set_ylabel(\"Val Accuracy\")\n",
    "    else:\n",
    "        ax1.plot(h[\"iter\"], np.arange(len(h)), marker=\"o\")\n",
    "        ax1.set_ylabel(\"Iteration index\")\n",
    "\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # secondary axis: pseudo-label growth if present\n",
    "    if \"new_pseudo\" in h.columns:\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(h[\"iter\"], h[\"new_pseudo\"], marker=\"s\")\n",
    "        ax2.set_ylabel(\"New pseudo-labels\")\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dynamics(h_self, \"Self-Training dynamics\")\n",
    "plot_dynamics(h_co, \"Co-Training dynamics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alerts per station (station-level storytelling)\n",
    "\n",
    "alerts_self = load_csv(SELF_ALERTS_PATH)\n",
    "alerts_co = load_csv(CO_ALERTS_PATH)\n",
    "\n",
    "def summarize_alerts(df: Optional[pd.DataFrame], name: str) -> Optional[pd.DataFrame]:\n",
    "    if df is None:\n",
    "        return None\n",
    "    if \"station\" not in df.columns or \"is_alert\" not in df.columns:\n",
    "        print(f\"{name}: missing required columns 'station'/'is_alert'. Columns:\", df.columns.tolist())\n",
    "        return None\n",
    "\n",
    "    d = df.copy()\n",
    "    d[\"is_alert\"] = pd.to_numeric(d[\"is_alert\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    g = (d.groupby(\"station\")\n",
    "           .agg(n=(\"is_alert\", \"size\"),\n",
    "                alert_count=(\"is_alert\", \"sum\"))\n",
    "           .reset_index())\n",
    "    g[\"alert_rate\"] = g[\"alert_count\"] / g[\"n\"].replace(0, np.nan)\n",
    "    g = g.sort_values([\"alert_count\", \"alert_rate\"], ascending=False)\n",
    "    print(f\"\\nTop alert stations — {name}\")\n",
    "    display(g.head(10))\n",
    "    return g\n",
    "\n",
    "sum_self = summarize_alerts(alerts_self, \"self_training\")\n",
    "sum_co = summarize_alerts(alerts_co, \"co_training\")\n",
    "\n",
    "def plot_top(g: Optional[pd.DataFrame], title: str):\n",
    "    if g is None or g.empty:\n",
    "        return\n",
    "    top = g.head(10)\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(top[\"station\"], top[\"alert_count\"].values)\n",
    "    ax.set_ylabel(\"Alert count\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticklabels(top[\"station\"], rotation=25, ha=\"right\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top(sum_self, \"Self-training: top-10 stations by alert count\")\n",
    "plot_top(sum_co, \"Co-training: top-10 stations by alert count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot station timeline: predicted class + alert markers (self-training first)\n",
    "\n",
    "def choose_station() -> Optional[str]:\n",
    "    if STATION_TO_PLOT is not None:\n",
    "        return str(STATION_TO_PLOT)\n",
    "\n",
    "    # auto: pick top-alert station from self, then co\n",
    "    for g in [sum_self, sum_co]:\n",
    "        if g is not None and not g.empty:\n",
    "            return str(g.iloc[0][\"station\"])\n",
    "    return None\n",
    "\n",
    "station = choose_station()\n",
    "print(\"Station to plot:\", station)\n",
    "\n",
    "def plot_station_timeline(df: Optional[pd.DataFrame], title: str):\n",
    "    if df is None or station is None:\n",
    "        return\n",
    "    if \"datetime\" not in df.columns:\n",
    "        print(f\"{title}: missing 'datetime' column.\")\n",
    "        return\n",
    "\n",
    "    d = df[df[\"station\"] == station].copy() if \"station\" in df.columns else df.copy()\n",
    "    if d.empty:\n",
    "        print(f\"{title}: no rows for station={station}\")\n",
    "        return\n",
    "\n",
    "    d[\"datetime\"] = pd.to_datetime(d[\"datetime\"], errors=\"coerce\")\n",
    "    d = d.dropna(subset=[\"datetime\"]).sort_values(\"datetime\").head(int(MAX_ROWS_PLOT))\n",
    "\n",
    "    # map class to rank for a clean line plot\n",
    "    order = [\"Good\",\"Moderate\",\"Unhealthy_for_Sensitive_Groups\",\"Unhealthy\",\"Very_Unhealthy\",\"Hazardous\"]\n",
    "    rank = {c:i for i,c in enumerate(order)}\n",
    "\n",
    "    pred_col = None\n",
    "    for c in [\"y_pred\", \"pred\", \"aqi_pred\", \"aqi_class_pred\"]:\n",
    "        if c in d.columns:\n",
    "            pred_col = c\n",
    "            break\n",
    "\n",
    "    if pred_col is None:\n",
    "        print(f\"{title}: cannot find prediction column (expected y_pred/pred/aqi_pred/aqi_class_pred). Columns:\", d.columns.tolist())\n",
    "        return\n",
    "\n",
    "    d[\"pred_rank\"] = d[pred_col].map(rank).astype(\"float64\")\n",
    "\n",
    "    if \"is_alert\" in d.columns:\n",
    "        is_alert = pd.to_numeric(d[\"is_alert\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "        d[\"alert_y\"] = np.where(is_alert.values == 1, d[\"pred_rank\"].values, np.nan)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(d[\"datetime\"], d[\"pred_rank\"], marker=\".\", linewidth=1)\n",
    "    if \"alert_y\" in d.columns:\n",
    "        ax.scatter(d[\"datetime\"], d[\"alert_y\"])\n",
    "    ax.set_yticks(list(rank.values()))\n",
    "    ax.set_yticklabels(order)\n",
    "    ax.set_xlabel(\"Datetime\")\n",
    "    ax.set_ylabel(\"Predicted AQI class\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_station_timeline(alerts_self, \"Self-training: predicted AQI class & alerts\")\n",
    "plot_station_timeline(alerts_co, \"Co-training: predicted AQI class & alerts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
